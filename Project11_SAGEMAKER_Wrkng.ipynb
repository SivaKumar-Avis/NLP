{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "### Loading the dataset (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "#filter out top 10000 used words\n",
    "vocab_size = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split ( 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "#load dataset as a list of ints\n",
    "# vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#Maximum sequence length\n",
    "#number of words used from each review\n",
    "maxlen = 300  \n",
    "\n",
    "#make all sequences of the same length using pad_sequences\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test =  pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "Jy6n-uM2eCy2",
    "outputId": "218e86b6-2190-4aa6-9d58-4e8fdcc0470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    1   43  188\n",
      "   46    5  566  264   51    6  530  664   14    9 1713   81   25 1135\n",
      "   46    7    6   20  750   11  141 4299    5    2 4441  102   28  413\n",
      "   38  120 5533   15    4 3974    7 5369  142  371  318    5  955 1713\n",
      "  571    2    2  122   14    8   72   54   12   86  385   46    5   14\n",
      "   20    9  399    8   72  150   13  161  124    6  155   44   14  159\n",
      "  170   83   12    5   51    6  866   48   25  842    4 1120   25  238\n",
      "   79    4  547   15   14    9   31    7  148    2  102   44   35  480\n",
      " 3823 2380   19  120    4  350  228    5  269    8   28  178 1314 2347\n",
      "    7   51    6   87   65   12    9  979   21   95   24 3186  178   11\n",
      "    2   14    9   24   15   20    4   84  376    4   65   14  127  141\n",
      "    6   52  292    7 4751  175  561    7   68 3866  137   75 2541   68\n",
      "  182    5  235  175  333   19   98   50    9   38   76  724    4 6750\n",
      "   15  166  285   36  140  143   38   76   53 3094 1301    4 6991   16\n",
      "   82    6   87 3578   44 2527 7612    5  800    4 3033   11   35 1728\n",
      "   96   21   14   22    9   76   53    7    6  406   65   13   43  219\n",
      "   12  639   21   13   80  140    5  135   15   14    9   31    7    4\n",
      "  118 3672   13   28  126  110] 1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[8],y_train[8])\n",
    "\n",
    "#Here the X_train is sequence representing the most commonly used words in the overall data say 1:1st commonly used word,171:171st commonly used word in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "fvI7tsJKFXNp",
    "outputId": "902c9038-101e-46a2-b5ac-ca6500812a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
      "  207  332 3356    7    2   44   14  500  112   38  729    8 2431 9256\n",
      "   54   13  256   12    4   86   58   13  197   12   16    6  227  916\n",
      "    2   40  500   19    6 1762    8 1677  125    4 2431 9256  405   24\n",
      "   43 3985   58   38   54   13  256   12   18    6  378    7  634   13\n",
      " 1706   89   76  253   12    9    5   89  275   39 2431 9256 1243    4\n",
      " 3985   58    9    6  227  729   21   13  104   12 2352 5897    8    4\n",
      "  500  405   14  500    9  701  570  206    6 1493  200    6    2   56\n",
      "    5    6    2   56   38   76  253   17    6  194  337    7 2431 9256\n",
      "   13  215  135   15    4  769    7    2    9   24  751    8    4 5058\n",
      "    7 2431 9256    4 2876   26    6  227  856    5   49    7    4 2191\n",
      "  168    4  172   21   48   25  181    6  227   53    7   15 3985   58\n",
      "   25  144  407  205   14  500] 1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[558],y_train[558])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model (30 points)\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [],
   "source": [
    "def create_seq_model():\n",
    "  model = Sequential()\n",
    "  #Here the 10000 is some random number, which is much larger than needed to reduce the probability of collisions from the hash function\n",
    "  #The number 10k should be greater than the total no of letters in each sequence\n",
    "  model.add(Embedding(10000,256,input_length=300))\n",
    "  model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "  model.add(GlobalMaxPool1D())\n",
    "  model.add(Dense(20, activation=\"relu\"))\n",
    "  model.add(Dropout(0.05))\n",
    "  model.add(Dense(1, activation=\"sigmoid\"))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Accuracy of the model  & Retrive the output of each layer in keras for a given single test sample from the trained model you built (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "-dUDSg7VeCzM",
    "outputId": "03aac925-f5ce-418d-99ad-77c1db9ffdb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300, 64)           73984     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 2,635,305\n",
      "Trainable params: 2,635,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "seq_nlp_model=create_seq_model()\n",
    "\n",
    "# summarize the model\n",
    "\n",
    "print(seq_nlp_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Tskt_1npeCzP",
    "outputId": "60d6392f-6ea9-476f-83b5-5d934f6cc4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "11895/20000 [================>.............] - ETA: 27:19 - loss: 0.3912 - acc: 0.8194"
     ]
    }
   ],
   "source": [
    "''' \n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "'''\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "epochs = 2\n",
    "\n",
    "# fit the model\n",
    "seq_nlp_model.fit(X_test,y_test, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIUe__knjQdy"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIjQ7hgLjTwS"
   },
   "outputs": [],
   "source": [
    "y_pred = seq_nlp_model.predict(X_test)\n",
    "\n",
    "\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
