{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SeqNLP_Project1_Questions_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "### Loading the dataset (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMdGMximJBJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "bf8964a2-4ce6-4ab0-b82c-65753eb59860"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "#filter out top 10000 used words\n",
        "vocab_size = 10000 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fCPC_WN-eCyw",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMEsHYrWxdtk"
      },
      "source": [
        "## Train test split ( 5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fFEzMzDJBJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13c75a27-2ad9-4497-b0f8-f91758b24940"
      },
      "source": [
        "#load dataset as a list of ints\n",
        "# vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
        "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h0g381XzeCyz",
        "colab": {}
      },
      "source": [
        "#Maximum sequence length\n",
        "#number of words used from each review\n",
        "maxlen = 300  \n",
        "\n",
        "#make all sequences of the same length using pad_sequences\n",
        "X_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "X_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jy6n-uM2eCy2",
        "outputId": "61ba4776-9c40-4e2f-edbb-0fb669aa7f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print(X_train[8],y_train[8])\n",
        "\n",
        "#Here the X_train is sequence representing the most commonly used words in the overall data say 1:1st commonly used word,171:171st commonly used word in the data."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    1   43  188\n",
            "   46    5  566  264   51    6  530  664   14    9 1713   81   25 1135\n",
            "   46    7    6   20  750   11  141 4299    5    2 4441  102   28  413\n",
            "   38  120 5533   15    4 3974    7 5369  142  371  318    5  955 1713\n",
            "  571    2    2  122   14    8   72   54   12   86  385   46    5   14\n",
            "   20    9  399    8   72  150   13  161  124    6  155   44   14  159\n",
            "  170   83   12    5   51    6  866   48   25  842    4 1120   25  238\n",
            "   79    4  547   15   14    9   31    7  148    2  102   44   35  480\n",
            " 3823 2380   19  120    4  350  228    5  269    8   28  178 1314 2347\n",
            "    7   51    6   87   65   12    9  979   21   95   24 3186  178   11\n",
            "    2   14    9   24   15   20    4   84  376    4   65   14  127  141\n",
            "    6   52  292    7 4751  175  561    7   68 3866  137   75 2541   68\n",
            "  182    5  235  175  333   19   98   50    9   38   76  724    4 6750\n",
            "   15  166  285   36  140  143   38   76   53 3094 1301    4 6991   16\n",
            "   82    6   87 3578   44 2527 7612    5  800    4 3033   11   35 1728\n",
            "   96   21   14   22    9   76   53    7    6  406   65   13   43  219\n",
            "   12  639   21   13   80  140    5  135   15   14    9   31    7    4\n",
            "  118 3672   13   28  126  110] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fvI7tsJKFXNp",
        "outputId": "e94400a4-7a4f-4cde-e2b9-4b244ff91a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print(X_train[558],y_train[558])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
            "  207  332 3356    7    2   44   14  500  112   38  729    8 2431 9256\n",
            "   54   13  256   12    4   86   58   13  197   12   16    6  227  916\n",
            "    2   40  500   19    6 1762    8 1677  125    4 2431 9256  405   24\n",
            "   43 3985   58   38   54   13  256   12   18    6  378    7  634   13\n",
            " 1706   89   76  253   12    9    5   89  275   39 2431 9256 1243    4\n",
            " 3985   58    9    6  227  729   21   13  104   12 2352 5897    8    4\n",
            "  500  405   14  500    9  701  570  206    6 1493  200    6    2   56\n",
            "    5    6    2   56   38   76  253   17    6  194  337    7 2431 9256\n",
            "   13  215  135   15    4  769    7    2    9   24  751    8    4 5058\n",
            "    7 2431 9256    4 2876   26    6  227  856    5   49    7    4 2191\n",
            "  168    4  172   21   48   25  181    6  227   53    7   15 3985   58\n",
            "   25  144  407  205   14  500] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dybtUgUReCy8"
      },
      "source": [
        "## Build Keras Embedding Layer Model (30 points)\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5OLM4eBeCy9",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxNDNhrseCzA",
        "colab": {}
      },
      "source": [
        "def create_seq_model():\n",
        "  model = Sequential()\n",
        "  #Here the 10000 is some random number, which is much larger than needed to reduce the probability of collisions from the hash function\n",
        "  #The number 10k should be greater than the total no of letters in each sequence\n",
        "  model.add(Embedding(10000,256,input_length=300))\n",
        "  model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  model.add(Dense(20, activation=\"relu\"))\n",
        "  model.add(Dropout(0.05))\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igq8Qm8GeCzG"
      },
      "source": [
        "## Accuracy of the model  & Retrive the output of each layer in keras for a given single test sample from the trained model you built (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-dUDSg7VeCzM",
        "outputId": "e9581337-00f7-42e5-f2f4-c8689bb10834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "seq_nlp_model=create_seq_model()\n",
        "\n",
        "# summarize the model\n",
        "\n",
        "print(seq_nlp_model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 300, 64)           73984     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 2,635,305\n",
            "Trainable params: 2,635,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tskt_1npeCzP",
        "outputId": "229e2b3d-ccb4-43b2-92eb-700cdf143aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "''' \n",
        "batch_size = 100\n",
        "epochs = 3\n",
        "'''\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "epochs = 3\n",
        "\n",
        "# fit the model\n",
        "seq_nlp_model.fit(X_test,y_test, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "  120/20000 [..............................] - ETA: 1:03:57 - loss: 0.6951 - acc: 0.4833"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rIUe__knjQdy",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JIjQ7hgLjTwS",
        "colab": {}
      },
      "source": [
        "y_pred = seq_nlp_model.predict(X_test)\n",
        "\n",
        "\n",
        "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
        "print('Confusion matrix:')\n",
        "confusion_matrix(y_pred, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsEC9bNJBKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ref URLs\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}